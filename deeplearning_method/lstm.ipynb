{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD8iESLfnEIe"
      },
      "outputs": [],
      "source": [
        "!pip install pandas keras tensorflow nltk scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hHf8n_I2nE56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-19 23:40:54.831699: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-09-19 23:40:54.835164: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-09-19 23:40:54.844279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-19 23:40:54.858253: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-19 23:40:54.862617: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-19 23:40:54.874695: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-19 23:40:56.395604: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "import nltk\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/workspaces/NLP-sesion')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xPoNkqmWrCoZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "from process_data import ProcessData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "74gx53TNnGgF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_114003/2831058031.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['sentiment'] = df['sentiment'].replace({'positive': 1, 'negative': 0})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16000 4000 16000 4000\n"
          ]
        }
      ],
      "source": [
        "df=pd.read_csv(\"/workspaces/NLP-sesion/IMDBDataset.csv\", nrows=20000)\n",
        "\n",
        "df['sentiment'] = df['sentiment'].replace({'positive': 1, 'negative': 0})\n",
        "processor = ProcessData(df, \"review\", \"sentiment\")\n",
        "\n",
        "df = processor.data_cleaning()\n",
        "X_train, X_test, y_train, y_test  = processor.data_split()\n",
        "print(len(X_train), len(X_test), len(y_train), len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab Size: 60714\n"
          ]
        }
      ],
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train['cleaned_review'])\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "print(f'Vocab Size: {vocab_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_words = 50\n",
        "\n",
        "X_train=pad_sequences(tokenizer.texts_to_sequences(X_train['cleaned_review']), maxlen=max_words, padding=\"post\")\n",
        "X_test=pad_sequences(tokenizer.texts_to_sequences(X_test['cleaned_review']), maxlen=max_words, padding=\"post\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  529,  4268,    17,    69,   945,  1203,    44,     5,    69,\n",
              "        3993,    35,    81,    67,    97,   204,     6,    83,    50,\n",
              "         630,    69,   324,   278,    21,    53,     1,  1477,     2,\n",
              "          96,    31,     4,  3582,  6453,  7477,    37,   707,  1489,\n",
              "          42,     4,  1397,    50,    27,     1,  2694,   950,   232,\n",
              "          41,   659,    42, 11244,  6203], dtype=int32)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5v1EnUH8qleO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-19 23:44:00.438829: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 24285600 exceeds 10% of free system memory.\n",
            "2024-09-19 23:44:00.503580: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 24285600 exceeds 10% of free system memory.\n",
            "2024-09-19 23:44:00.517231: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 24285600 exceeds 10% of free system memory.\n",
            "2024-09-19 23:44:00.569245: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 24285600 exceeds 10% of free system memory.\n",
            "2024-09-19 23:44:00.588738: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 24285600 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 71ms/step - accuracy: 0.7002 - loss: 0.5575 - val_accuracy: 0.8108 - val_loss: 0.4072\n",
            "Epoch 2/2\n",
            "\u001b[1m5334/5334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 73ms/step - accuracy: 0.8906 - loss: 0.2627 - val_accuracy: 0.8217 - val_loss: 0.4260\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x717553cb8680>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an LSTM model with an Embedding layer and fit training data\n",
        "model=Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size,output_dim=100))\n",
        "model.add(layers.Bidirectional(layers.LSTM(128)))\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='adam',loss='BinaryCrossentropy',metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=3, epochs=2,validation_data=(X_test,y_test), verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    it be not painful to watch , actually i think it be an ok movie\n",
            "Name: cleaned_review, dtype: object\n"
          ]
        }
      ],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "sentence_to_test = \"It was not painful to watch, actually i thought it was an ok movie\"\n",
        "sentiment = 1\n",
        "\n",
        "df = pd.DataFrame([[sentence_to_test, sentiment]], columns=['review', 'sentiment'])\n",
        "test_processor = ProcessData(df, \"review\", \"sentiment\")\n",
        "\n",
        "cleaned_sentence = test_processor.data_cleaning()\n",
        "sentence_seq = tokenizer.texts_to_sequences(cleaned_sentence[\"cleaned_review\"])  # convert to sequence\n",
        "preprocessed_sentence = pad_sequences(sentence_seq, maxlen=50, padding=\"post\")\n",
        "len(preprocessed_sentence)\n",
        "print(cleaned_sentence['cleaned_review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[9, 2, 21, 1301, 6, 58, 161, 10, 71, 9, 2, 32, 594, 14]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_seq[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "yQ2vPZ_8q0F1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.05127384], dtype=float32)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make a prediction\n",
        "prediction = model(preprocessed_sentence, training=False)\n",
        "\n",
        "prediction.numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
