{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD8iESLfnEIe"
      },
      "outputs": [],
      "source": [
        "!pip install pandas keras tensorflow nltk scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHf8n_I2nE56"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPoNkqmWrCoZ"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74gx53TNnGgF"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/IMDBDataset.csv', nrows=2500)\n",
        "\n",
        "df['sentiment'] = df['sentiment'].replace({'positive': 1, 'negative': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v1EnUH8qleO"
      },
      "outputs": [],
      "source": [
        "# Create all the features to the data set\n",
        "def data_cleaning(text_list):\n",
        "    stopwords_rem=False\n",
        "    stopwords_en=stopwords.words('english')\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    tokenizer=TweetTokenizer()\n",
        "    reconstructed_list=[]\n",
        "    for each_text in text_list:\n",
        "        lemmatized_tokens=[]\n",
        "        tokens=tokenizer.tokenize(each_text.lower())\n",
        "        pos_tags=pos_tag(tokens)\n",
        "        for each_token, tag in pos_tags:\n",
        "            if tag.startswith('NN'):\n",
        "                pos='n'\n",
        "            elif tag.startswith('VB'):\n",
        "                pos='v'\n",
        "            else:\n",
        "                pos='a'\n",
        "            lemmatized_token=lemmatizer.lemmatize(each_token, pos)\n",
        "            if stopwords_rem: # False\n",
        "                if lemmatized_token not in stopwords_en:\n",
        "                    lemmatized_tokens.append(lemmatized_token)\n",
        "            else:\n",
        "                lemmatized_tokens.append(lemmatized_token)\n",
        "        reconstructed_list.append(' '.join(lemmatized_tokens))\n",
        "    return reconstructed_list\n",
        "\n",
        "\n",
        "# Break data down into a training set and a testing set\n",
        "X=df['review']\n",
        "y=df['sentiment']\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, y)\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train=data_cleaning(X_train)\n",
        "X_test=data_cleaning(X_test)\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "print(f'Vocab Size: {vocab_size}')\n",
        "X_train=pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=40)\n",
        "X_test=pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=40)\n",
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)\n",
        "\n",
        "# Create an LSTM model with an Embedding layer and fit training data\n",
        "model=Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size,\\\n",
        "                           output_dim=100,\\\n",
        "                           input_length=40))\n",
        "model.add(layers.Bidirectional(layers.LSTM(128)))\n",
        "model.add(layers.Dense(2,activation='softmax'))\n",
        "model.compile(optimizer='adam',\\\n",
        "              loss='categorical_crossentropy',\\\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=256, epochs=10,validation_data=(X_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ2vPZ_8q0F1"
      },
      "outputs": [],
      "source": [
        "# New input string (e.g., a review)\n",
        "new_sentence = \"I really hated the movie, it was awful!\"\n",
        "\n",
        "# Preprocess the input string (similar to your `data_cleaning` function)\n",
        "def preprocess_input(sentence):\n",
        "    # Tokenize and lemmatize the input string\n",
        "    sentence_cleaned = data_cleaning([sentence])  # using the same cleaning function as before\n",
        "\n",
        "    # Tokenize and pad the sentence\n",
        "    sentence_seq = tokenizer.texts_to_sequences(sentence_cleaned)  # convert to sequence\n",
        "    sentence_padded = pad_sequences(sentence_seq, maxlen=40)  # pad the sequence to maxlen 40\n",
        "\n",
        "    return sentence_padded\n",
        "\n",
        "# Preprocess the input sentence\n",
        "preprocessed_sentence = preprocess_input(new_sentence)\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(preprocessed_sentence)\n",
        "\n",
        "# Get the predicted class (0 or 1, depending on your sentiment classification)\n",
        "predicted_class = np.argmax(prediction, axis=-1)\n",
        "\n",
        "# Interpret the result\n",
        "if predicted_class == 1:\n",
        "    print(\"Positive sentiment\")\n",
        "else:\n",
        "    print(\"Negative sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8qI3N9a0X_d"
      },
      "outputs": [],
      "source": [
        "input_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYKszTuJz3Xw"
      },
      "outputs": [],
      "source": [
        "p = model.predict(input_padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpiudX030Ee2"
      },
      "outputs": [],
      "source": [
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bd2zjQ90G7W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.argmax(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG-k40Py0QlW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
