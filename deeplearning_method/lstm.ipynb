{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD8iESLfnEIe"
      },
      "outputs": [],
      "source": [
        "!pip install pandas keras tensorflow nltk scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hHf8n_I2nE56"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/workspaces/NLP-sesion')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xPoNkqmWrCoZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-19 09:42:34.125611: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-09-19 09:42:34.355479: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-09-19 09:42:34.536464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-19 09:42:34.840860: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-19 09:42:34.933739: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-19 09:42:35.415620: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-19 09:42:37.164940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from process_data import ProcessData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "74gx53TNnGgF"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspaces/NLP-sesion/IMDB Dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m)\n\u001b[1;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "df=pd.read_csv(\"/workspaces/NLP-sesion/IMDB Dataset.csv\", nrows=2500)\n",
        "\n",
        "df['sentiment'] = df['sentiment'].replace({'positive': 1, 'negative': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ProcessData' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mProcessData\u001b[49m(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ProcessData' is not defined"
          ]
        }
      ],
      "source": [
        "processor = ProcessData(df, \"review\", \"sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v1EnUH8qleO"
      },
      "outputs": [],
      "source": [
        "processor = ProcessData(df, \"review\", \"sentiment\")\n",
        "\n",
        "\n",
        "# Create all the features to the data set\n",
        "def data_cleaning(text_list):\n",
        "    stopwords_rem=False\n",
        "    stopwords_en=stopwords.words('english')\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    tokenizer=TweetTokenizer()\n",
        "    reconstructed_list=[]\n",
        "    for each_text in text_list:\n",
        "        lemmatized_tokens=[]\n",
        "        tokens=tokenizer.tokenize(each_text.lower())\n",
        "        pos_tags=pos_tag(tokens)\n",
        "        for each_token, tag in pos_tags:\n",
        "            if tag.startswith('NN'):\n",
        "                pos='n'\n",
        "            elif tag.startswith('VB'):\n",
        "                pos='v'\n",
        "            else:\n",
        "                pos='a'\n",
        "            lemmatized_token=lemmatizer.lemmatize(each_token, pos)\n",
        "            if stopwords_rem: # False\n",
        "                if lemmatized_token not in stopwords_en:\n",
        "                    lemmatized_tokens.append(lemmatized_token)\n",
        "            else:\n",
        "                lemmatized_tokens.append(lemmatized_token)\n",
        "        reconstructed_list.append(' '.join(lemmatized_tokens))\n",
        "    return reconstructed_list\n",
        "\n",
        "\n",
        "# Break data down into a training set and a testing set\n",
        "X=df['review']\n",
        "y=df['sentiment']\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, y)\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train=data_cleaning(X_train)\n",
        "X_test=data_cleaning(X_test)\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "print(f'Vocab Size: {vocab_size}')\n",
        "X_train=pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=40)\n",
        "X_test=pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=40)\n",
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)\n",
        "\n",
        "# Create an LSTM model with an Embedding layer and fit training data\n",
        "model=Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size,\\\n",
        "                           output_dim=100,\\\n",
        "                           input_length=40))\n",
        "model.add(layers.Bidirectional(layers.LSTM(128)))\n",
        "model.add(layers.Dense(2,activation='softmax'))\n",
        "model.compile(optimizer='adam',\\\n",
        "              loss='categorical_crossentropy',\\\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=256, epochs=10,validation_data=(X_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ2vPZ_8q0F1"
      },
      "outputs": [],
      "source": [
        "# New input string (e.g., a review)\n",
        "new_sentence = \"I really hated the movie, it was awful!\"\n",
        "\n",
        "# Preprocess the input string (similar to your `data_cleaning` function)\n",
        "def preprocess_input(sentence):\n",
        "    # Tokenize and lemmatize the input string\n",
        "    sentence_cleaned = data_cleaning([sentence])  # using the same cleaning function as before\n",
        "\n",
        "    # Tokenize and pad the sentence\n",
        "    sentence_seq = tokenizer.texts_to_sequences(sentence_cleaned)  # convert to sequence\n",
        "    sentence_padded = pad_sequences(sentence_seq, maxlen=40)  # pad the sequence to maxlen 40\n",
        "\n",
        "    return sentence_padded\n",
        "\n",
        "# Preprocess the input sentence\n",
        "preprocessed_sentence = preprocess_input(new_sentence)\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(preprocessed_sentence)\n",
        "\n",
        "# Get the predicted class (0 or 1, depending on your sentiment classification)\n",
        "predicted_class = np.argmax(prediction, axis=-1)\n",
        "\n",
        "# Interpret the result\n",
        "if predicted_class == 1:\n",
        "    print(\"Positive sentiment\")\n",
        "else:\n",
        "    print(\"Negative sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8qI3N9a0X_d"
      },
      "outputs": [],
      "source": [
        "input_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYKszTuJz3Xw"
      },
      "outputs": [],
      "source": [
        "p = model.predict(input_padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpiudX030Ee2"
      },
      "outputs": [],
      "source": [
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bd2zjQ90G7W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.argmax(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG-k40Py0QlW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
